# -*- coding: utf-8 -*-
"""InvestiData2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/154ZRsJmuqyrc5YizTmUvI6K1TRMMBO_N
"""


## configurar Ngrok
from pyngrok import ngrok

NGROK_TOKEN = "35lDF3fBUKFBddtPi8rq4Nuw6JM_5AJq5XCxPYyXobnGPXJot"
ngrok.set_auth_token(NGROK_TOKEN)

public_url = ngrok.connect(8501)
public_url
# Crear t√∫nel para puerto 8501 (Streamlit)
public_url = ngrok.connect(8501)

print("üöÄ Tu app est√° disponible en:")
print(public_url)

# Commented out IPython magic to ensure Python compatibility.
# ## crear streamlit
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import networkx as nx
# import matplotlib.pyplot as plt
# from collections import Counter
# import re
# 
# # RUTA LOCAL (archivo que subiste)
# DEFAULT_FILE = "/mnt/data/202500019_2025-06-26_Informe.xlsx"
# 
# st.set_page_config(page_title="InvestiData - Anal√≠tica de Extracciones Forenses", layout="wide")
# 
# # --- HEADER ---
# st.title("üìä InvestiData ‚Äì Anal√≠tica de Extracciones Forenses")
# st.write("Mapas, Nombres propios, Detecci√≥n de patrones y Grafo de contactos. Usa el uploader o el archivo ya cargado.")
# 
# # --- MODO OSCURO ---
# dark_mode = st.checkbox("üåô Modo oscuro")
# if dark_mode:
#     st.markdown(
#         """
#         <style>
#         .reportview-container {background: #111213;}
#         .stApp {background: #111213; color: #e6e6e6;}
#         .css-1d391kg {color: #e6e6e6;}
#         .stButton>button {background-color:#2b2b2b;color:#fff}
#         .stTextInput>div>div>input {background:#222;color:#eee}
#         </style>
#         """, unsafe_allow_html=True
#     )
# 
# # --- SUBIR ARCHIVO (o usar por defecto) ---
# uploaded_file = st.file_uploader("üìÇ Cargar archivo XLSX (opcional)", type=["xlsx"])
# use_default = st.checkbox("Usar archivo integrado de la sesi√≥n (si no subo archivo)", value=True)
# # ============================================================
# #   PERFIL DEL DISPOSITIVO A LA DERECHA
# # ============================================================
# 
# if uploaded_file is not None:
# 
#     col_left, col_right = st.columns([3, 1])
# 
#     with col_right:
#         st.markdown("### üü¶ Perfil del Dispositivo")
#         st.markdown(
#             """
#             <div style="font-size: 13px; line-height: 1.3; padding: 8px;
#                         border: 1px solid #2980b9; border-radius: 8px;">
#             üì± <b>Moto G24 ‚Äì Motorola</b><br>
#             üìß <b>herreragus1976@gmail.com</b><br>
#             üí¨ <b>+57 311 252 8641</b> ‚Äì ‚ÄúCalvin Klein‚Äù<br>
#             üî¢ <b>IMEI:</b><br>
#             &nbsp;&nbsp;‚Ä¢ 354102943867594<br>
#             &nbsp;&nbsp;‚Ä¢ 354102943902490<br>
#             üé® <b>Color:</b> Gris<br>
#             üß© <b>SIM:</b> 1 encontrada<br>
#             üõ† <b>Estado:</b> Bueno<br>
#             </div>
#             """,
#             unsafe_allow_html=True
#         )
# 
#     with col_left:
#         st.success("‚úî Archivo cargado correctamente. Contin√∫a con el an√°lisis‚Ä¶")
# 
# if uploaded_file is None and not use_default:
#     st.info("Sube un archivo XLSX o marca 'Usar archivo integrado' para continuar.")
#     st.stop()
# 
# file_path = None
# if uploaded_file is not None:
#     file_source = "uploaded"
#     # pandas aceptar√° el file-like object
#     try:
#         xls = pd.ExcelFile(uploaded_file)
#         st.success("‚úî Archivo subido correctamente.")
#     except Exception as e:
#         st.error(f"No se pudo leer el archivo subido: {e}")
#         st.stop()
# else:
#     # usar el archivo por defecto en disco
#     file_source = "default"
#     try:
#         xls = pd.ExcelFile(DEFAULT_FILE)
#         st.success(f"‚úî Archivo por defecto cargado: {DEFAULT_FILE}")
#     except Exception as e:
#         st.error(f"No se pudo leer el archivo por defecto: {e}")
#         st.stop()
# 
# # --- SELECCI√ìN DE HOJA ---
# hoja = st.selectbox("üìë Selecciona la hoja a analizar", xls.sheet_names)
# df = xls.parse(hoja)
# st.markdown("### üëÄ Vista previa")
# st.dataframe(df.head())
# 
# # Normalizar nombres de columnas a min√∫sculas sin espacios
# df.columns = [c.strip() for c in df.columns]
# cols_lower = {c: c.lower() for c in df.columns}
# df.rename(columns=cols_lower, inplace=True)
# 
# # --- FILTROS R√ÅPIDOS (fila) ---
# st.markdown("---")
# st.subheader("üéØ Filtros r√°pidos")
# c1, c2, c3, c4 = st.columns(4)
# with c1:
#     keyword = st.text_input("üîç Palabra clave")
# with c2:
#     if "fecha" in df.columns:
#         df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce")
#         min_f = df["fecha"].min()
#         max_f = df["fecha"].max()
#         fecha_inicio = st.date_input("üìÖ Desde", min_f)
#     else:
#         fecha_inicio = None
# with c3:
#     if "fecha" in df.columns:
#         fecha_fin = st.date_input("üìÖ Hasta", max_f)
#     else:
#         fecha_fin = None
# with c4:
#     contact_filter = st.text_input("üë§ Contacto / N√∫mero")
# 
# # Aplicar filtros manuales
# df_filtered = df.copy()
# if keyword:
#     df_filtered = df_filtered[df_filtered.astype(str).apply(lambda row: row.str.contains(keyword, case=False).any(), axis=1)]
# if "fecha" in df_filtered.columns and fecha_inicio and fecha_fin:
#     df_filtered = df_filtered[(df_filtered["fecha"] >= pd.to_datetime(fecha_inicio)) & (df_filtered["fecha"] <= pd.to_datetime(fecha_fin))]
# if contact_filter:
#     df_filtered = df_filtered[df_filtered.astype(str).apply(lambda row: row.str.contains(contact_filter, case=False).any(), axis=1)]
# 
# st.markdown("### üìã Resultados filtrados (primeros 200 registros)")
# st.dataframe(df_filtered.head(200))
# 
# # --- M√ìDULO: DETECCI√ìN DE NOMBRES PROPIOS (heur√≠stica) ---
# st.markdown("---")
# st.subheader("üßæ Nombres propios detectados (heur√≠stica)")
# text_columns = [c for c in df_filtered.columns if df_filtered[c].dtype == object or df_filtered[c].dtype == "string"]
# combined_text = " ".join(df_filtered[text_columns].astype(str).values.flatten())
# # Extrae tokens que comienzan con may√∫scula y al menos 2 letras, evitando inicio de frases comunes
# tokens = re.findall(r"\b[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]{2,}\b", combined_text)
# name_counts = Counter(tokens).most_common(20)
# st.write(pd.DataFrame(name_counts, columns=["Nombre", "Frecuencia"]))
# 
# # --- M√ìDULO: DETECCI√ìN DE PATRONES Y SUSPICIOSOS ---
# st.markdown("---")
# st.subheader("üö® Detecci√≥n de patrones sospechosos")
# # keywords de delitos
# keywords = ["arma","pistola","rev√≥lver","droga","matar","ni√±a","extorsi√≥n","vacuna","pagar","amenaza"]
# patron = "|".join(keywords)
# df_sospechoso = df_filtered[df_filtered.astype(str).apply(lambda row: row.str.contains(patron, case=False).any(), axis=1)]
# st.write(f"Registros que contienen palabras claves ({len(df_sospechoso)}):")
# st.dataframe(df_sospechoso.head(200))
# 
# # detectar n√∫meros (tel√©fonos) y ver repetidos
# all_text = " ".join(df_filtered.astype(str).values.flatten())
# numbers = re.findall(r"\b(?:\+?\d{7,15}|\d{7,15})\b", all_text)
# num_counts = Counter(numbers).most_common(10)
# st.markdown("### üî¢ N√∫meros detectados m√°s comunes")
# st.write(pd.DataFrame(num_counts, columns=["N√∫mero", "Frecuencia"]))
# 
# # --- M√ìDULO: MAPA (si hay coordenadas) ---
# st.markdown("---")
# st.subheader("üó∫Ô∏è Mapa de ubicaciones (si existen columnas de lat/lon)")
# # posibles nombres de columnas de lat/lon
# lat_cols = [c for c in df_filtered.columns if "lat" in c]
# lon_cols = [c for c in df_filtered.columns if "lon" in c or "long" in c]
# if lat_cols and lon_cols:
#     lat_col = lat_cols[0]
#     lon_col = lon_cols[0]
#     map_df = df_filtered[[lat_col, lon_col]].dropna()
#     # st.map espera columnas lat/lon con nombres 'lat'/'lon' -> renombrar temporalmente
#     map_df = map_df.rename(columns={lat_col: "lat", lon_col: "lon"})
#     # st.map requires lat/lon in floats
#     try:
#         map_df["lat"] = map_df["lat"].astype(float)
#         map_df["lon"] = map_df["lon"].astype(float)
#         st.map(map_df)
#     except:
#         st.info("Las columnas de coordenadas existen pero no est√°n en formato num√©rico.")
# else:
#     st.info("No se detectaron columnas de latitud/longitud en la hoja (busque columnas que contengan 'lat' y 'lon').")
# 
# # --- M√ìDULO: GRAFO DE CONTACTOS (si hay remitente y receptor) ---
# st.markdown("---")
# st.subheader("üîó Grafo de contactos (remitente -> receptor)")
# 
# sender_cols = [c for c in df_filtered.columns if "remit" in c]  # remitente, sender
# receiver_cols = [c for c in df_filtered.columns if "recept" in c or "receptor" in c or "to" == c]
# # fallback typical names
# if not sender_cols and "from" in df_filtered.columns:
#     sender_cols = ["from"]
# if not receiver_cols and "to" in df_filtered.columns:
#     receiver_cols = ["to"]
# 
# if sender_cols and receiver_cols:
#     s = sender_cols[0]
#     r = receiver_cols[0]
#     # build edges
#     edges = df_filtered[[s, r]].dropna().astype(str).values.tolist()
#     G = nx.DiGraph()
#     G.add_edges_from([(a, b) for a, b in edges])
#     # calculate degree centrality
#     centrality = nx.degree_centrality(G)
#     # top nodes
#     top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]
#     st.markdown("Top nodos (grado centralidad):")
#     st.write(pd.DataFrame(top_nodes, columns=["Nodo", "Centralidad"]))
#     # draw graph (matplotlib)
#     fig, ax = plt.subplots(figsize=(7, 5))
#     pos = nx.spring_layout(G, k=0.5, iterations=30)
#     nx.draw_networkx_nodes(G, pos, node_size=50, ax=ax)
#     nx.draw_networkx_edges(G, pos, alpha=0.3, ax=ax)
#     # label only top nodes
#     labels = {n: n for n, _ in top_nodes}
#     nx.draw_networkx_labels(G, pos, labels, font_size=8, ax=ax)
#     ax.set_title("Grafo de contactos (muestra)")
#     ax.axis("off")
#     st.pyplot(fig)
# else:
#     st.info("No se detectaron columnas claras de remitente/receptor para construir grafo.")
# 
# # --- SECCI√ìN: INFORMACI√ìN DEL PROPIETARIO (si existe alguna hoja con 'propiet' o 'device') ---
# st.markdown("---")
# st.subheader("üë§ Informaci√≥n probable del propietario del dispositivo")
# owner_sheets = [s for s in xls.sheet_names if any(k in s.lower() for k in ["propiet", "device", "inform", "owner"])]
# if owner_sheets:
#     st.write("Hoja(s) detectada(s) con posible informaci√≥n del propietario:")
#     st.write(owner_sheets)
#     for s in owner_sheets:
#         st.markdown(f"**Hoja: {s}**")
#         df_owner = xls.parse(s)
#         st.dataframe(df_owner.head(30))
# else:
#     st.info("No se encontr√≥ una hoja que claramente identifique al propietario (busca nombres como 'Propietario', 'Device Info', 'Informaci√≥n').")
# 
# # --- DESCARGA DEL CSV FILTRADO ---
# st.markdown("---")
# st.download_button(
#     "‚¨áÔ∏è Descargar CSV filtrado",
#     df_filtered.to_csv(index=False).encode("utf-8"),
#     "resultados_filtrados_avanzado.csv",
#     mime="text/csv"
# )
# 
# st.success("An√°lisis completado ‚úÖ")
# 
#

## ejecutar la app
!streamlit run app.py & sleep 5
